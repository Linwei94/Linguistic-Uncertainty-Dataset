name: lora
r: 4
target_modules:
  - q_proj
  - v_proj
lora_alpha: 32
lora_dropout: 0.05

output_dir: build/output/model/qwen3_8b_lora
learning_rate: 1e-3
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
num_train_epochs: 2
eval_strategy: epoch
save_strategy: epoch
load_best_model_at_end: true
bf16: true
fp16: false