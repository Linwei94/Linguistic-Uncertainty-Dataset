{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniformly select 200 examples from original SimpleQA dataset\n",
    "1. download original SimpleQA dataset from this [link](https://huggingface.co/datasets/basicv8vc/SimpleQA/resolve/main/simple_qa_test_set.csv?download=true)\n",
    "2. run the following code to select 200 examples uniformly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "#set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>{'topic': 'Art', 'answer_type': 'Number', 'url...</td>\n",
       "      <td>At what age was Ken Noda invited by President ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>{'topic': 'Art', 'answer_type': 'Other', 'urls...</td>\n",
       "      <td>Which art dealership did Peter Arrell Browne W...</td>\n",
       "      <td>Knoedler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>{'topic': 'Music', 'answer_type': 'Person', 'u...</td>\n",
       "      <td>What was composer Sigrid Ingeborg Henriette Wi...</td>\n",
       "      <td>Anna Bruun Tordenskjold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3524</th>\n",
       "      <td>{'topic': 'Geography', 'answer_type': 'Number'...</td>\n",
       "      <td>What is the forest cover area of Madhya Prades...</td>\n",
       "      <td>77,482.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>{'topic': 'TV shows', 'answer_type': 'Person',...</td>\n",
       "      <td>Who kills Daryl Garrs in Happy Valley?</td>\n",
       "      <td>Alison Garrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               metadata  \\\n",
       "2333  {'topic': 'Art', 'answer_type': 'Number', 'url...   \n",
       "803   {'topic': 'Art', 'answer_type': 'Other', 'urls...   \n",
       "3113  {'topic': 'Music', 'answer_type': 'Person', 'u...   \n",
       "3524  {'topic': 'Geography', 'answer_type': 'Number'...   \n",
       "1675  {'topic': 'TV shows', 'answer_type': 'Person',...   \n",
       "\n",
       "                                                problem  \\\n",
       "2333  At what age was Ken Noda invited by President ...   \n",
       "803   Which art dealership did Peter Arrell Browne W...   \n",
       "3113  What was composer Sigrid Ingeborg Henriette Wi...   \n",
       "3524  What is the forest cover area of Madhya Prades...   \n",
       "1675             Who kills Daryl Garrs in Happy Valley?   \n",
       "\n",
       "                       answer  \n",
       "2333                       20  \n",
       "803                  Knoedler  \n",
       "3113  Anna Bruun Tordenskjold  \n",
       "3524                77,482.49  \n",
       "1675             Alison Garrs  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df = pd.read_csv('simple_qa_test_set.csv')\n",
    "\n",
    "# uniformly sample 200 rows from the original DataFrame\n",
    "sampled_df = original_df.sample(n=200, random_state=42)\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLM APIs to generate uncertain statements using instruction template\n",
    "Here, we use `build_dataset.py` to generate uncertain statements using LLM APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction template\n",
    "instruction_template = \"\"\"\n",
    "You are given a question and its ground-truth answer. Your task is to generate 50 answer sentences that express the same answer using different levels of confidence:\n",
    "\n",
    "10 with high confidence\n",
    "10 with moderate confidence\n",
    "10 with low confidence\n",
    "10 with lowest confidence\n",
    "10 with complete uncertainty, reject to reply\n",
    "\n",
    "The wording should vary across the levels, but all responses should convey the same core answer. Focus on natural and diverse expressions of confidence.\n",
    "\n",
    "Question: {}\n",
    "Answer: {}\n",
    "\"\"\"\n",
    "\n",
    "# llm api keys\n",
    "llm_api_keys = {\n",
    "    'gpt': os.getenv('GPT_API_KEY'),\n",
    "    'gemini': os.getenv('GEMINI_API_KEY'),\n",
    "    'claude': os.getenv('CLAUDE_API_KEY'),\n",
    "    'grok': os.getenv('GROK_API_KEY')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 10,000 uncertain statements per LLM\n",
    "Each LLM can take around 2 hours to generate, you can also use `sampling_response.py` to sample the generated uncertain statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_api_handler import gemini_generate, gpt_generate, claude_generate, grok_generate\n",
    "\n",
    "\n",
    "llm_to_use = 'gemini'  # Change this to 'gpt', 'gemini', 'gemini' or 'grok' as needed\n",
    "llm_generate = {\n",
    "    'gpt': gpt_generate, # gpt-4.1\n",
    "    'gemini': gemini_generate, # gemini-2.5-pro\n",
    "    'claude': claude_generate, # claude-sonnet-4-20250514\n",
    "    'grok': grok_generate # grok-3\n",
    "}[llm_to_use]\n",
    "\n",
    "# # uncomment to test api\n",
    "# first_row = sampled_df.iloc[0]\n",
    "# instruction = instruction_template.format(first_row['problem'], first_row['answer'])\n",
    "# response = llm_generate(instruction, llm_api_keys[llm_to_use])\n",
    "# print(response)\n",
    "\n",
    "# new_df = pd.DataFrame(columns=['problem', 'answer', 'raw_response', 'metadata'])\n",
    "\n",
    "# for index, row in tqdm(sampled_df.iterrows()):\n",
    "#     problem = row['problem']\n",
    "#     answer = row['answer']\n",
    "#     metadata = row['metadata']\n",
    "#     instruction = instruction_template.format(problem, answer)\n",
    "#     text = llm_generate(instruction, api_key=llm_api_keys[llm_to_use])\n",
    "#     # add the generated text to the new df\n",
    "#     new_row = {\n",
    "#         'problem': problem,\n",
    "#         'answer': answer,\n",
    "#         'raw_response': text,\n",
    "#         'metadata': metadata\n",
    "#     }\n",
    "#     new_df = new_df._append(new_row, ignore_index=True)\n",
    "#     # save the new df to csv file\n",
    "#     new_df.to_csv(f'generated_text_{llm_to_use}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract uncertain statements from raw reponse using regex\n",
    "use interactive code to modify edge cases by running \n",
    "`python extract_sentences.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 10,000 uncertain statements from the extracted uncertain statements\n",
    "Extract selected 2500 examples per LLM and make a new df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "grok      2701\n",
       "gemini    2492\n",
       "claude    2445\n",
       "gpt       2362\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "extracted_df = pd.read_csv(f'all_sentences_by_confidence.csv')\n",
    "\n",
    "# Save the index of extracted_df as orig_idx\n",
    "extracted_df = extracted_df.reset_index().rename(columns={'index': 'orig_idx'})\n",
    "\n",
    "# We want to sample 10000 unique sentences, stratified by confidence level.\n",
    "# For 'completely uncertain', sample 5% (at least 1), for others, sample uniformly so that total is 10000.\n",
    "# If there are duplicate sentences, resample to ensure all 10000 are unique.\n",
    "\n",
    "total_n = 10000\n",
    "sampled_sentences = set()\n",
    "sampled_rows = []\n",
    "\n",
    "# Step 1: Sample 'completely uncertain'\n",
    "conf_levels = list(extracted_df['confidence'].unique())\n",
    "completely_uncertain_conf = 'completely uncertain'\n",
    "other_levels = [c for c in conf_levels if c != completely_uncertain_conf]\n",
    "\n",
    "conf_df = extracted_df[extracted_df['confidence'] == completely_uncertain_conf]\n",
    "n_cu = int(len(conf_df) * 0.0628)\n",
    "n_cu = max(n_cu, 1)\n",
    "cu_sample = conf_df.sample(n=n_cu, random_state=42)\n",
    "# Remove duplicates by sentence\n",
    "cu_sample = cu_sample.drop_duplicates(subset=['sentence'])\n",
    "sampled_rows.append(cu_sample)\n",
    "sampled_sentences.update(cu_sample['sentence'].tolist())\n",
    "completely_uncertain_n = len(cu_sample)\n",
    "\n",
    "# Step 2: Sample from other levels, ensuring uniqueness\n",
    "remaining_n = total_n - completely_uncertain_n\n",
    "n_per_level = remaining_n // len(other_levels)\n",
    "extra = remaining_n % len(other_levels)\n",
    "\n",
    "for i, conf in enumerate(other_levels):\n",
    "    conf_df = extracted_df[extracted_df['confidence'] == conf]\n",
    "    n = n_per_level + (1 if i < extra else 0)\n",
    "    # Remove sentences already sampled\n",
    "    conf_df = conf_df[~conf_df['sentence'].isin(sampled_sentences)]\n",
    "    # If not enough left, just take all\n",
    "    n = min(n, len(conf_df))\n",
    "    # Sample, drop duplicates, and ensure uniqueness\n",
    "    sampled = pd.DataFrame()\n",
    "    attempts = 0\n",
    "    while len(sampled) < n and attempts < 20:\n",
    "        needed = n - len(sampled)\n",
    "        # Oversample a bit to account for possible duplicates\n",
    "        try_sample = conf_df.sample(n=min(needed*2, len(conf_df)), random_state=42+attempts)\n",
    "        try_sample = try_sample[~try_sample['sentence'].isin(sampled_sentences)]\n",
    "        try_sample = try_sample.drop_duplicates(subset=['sentence'])\n",
    "        sampled = pd.concat([sampled, try_sample]).drop_duplicates(subset=['sentence'])\n",
    "        sampled = sampled.head(n)\n",
    "        attempts += 1\n",
    "    sampled_rows.append(sampled)\n",
    "    sampled_sentences.update(sampled['sentence'].tolist())\n",
    "\n",
    "# Step 3: Combine and check for uniqueness\n",
    "sampled_10000_df = pd.concat(sampled_rows, ignore_index=True)\n",
    "sampled_10000_df = sampled_10000_df.drop_duplicates(subset=['sentence']).reset_index(drop=True)\n",
    "\n",
    "# If we have less than 10000 due to deduplication, fill up with random unique sentences from the rest\n",
    "if len(sampled_10000_df) < total_n:\n",
    "    needed = total_n - len(sampled_10000_df)\n",
    "    remaining_df = extracted_df[~extracted_df['sentence'].isin(sampled_10000_df['sentence'])]\n",
    "    fill_sample = remaining_df.drop_duplicates(subset=['sentence']).sample(n=needed, random_state=123)\n",
    "    sampled_10000_df = pd.concat([sampled_10000_df, fill_sample], ignore_index=True)\n",
    "    sampled_10000_df = sampled_10000_df.drop_duplicates(subset=['sentence']).reset_index(drop=True)\n",
    "\n",
    "# Final check\n",
    "assert sampled_10000_df['sentence'].is_unique\n",
    "assert len(sampled_10000_df) == 10000\n",
    "\n",
    "sampled_10000_df['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence\n",
       "high                    2375\n",
       "low                     2375\n",
       "moderate                2375\n",
       "lowest                  2375\n",
       "completely uncertain     500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_10000_df['confidence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_10000_df['sentence'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_10000_df.to_csv('sampled_10000_sentences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make mturk dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 50 rows from the sampled_10000_df\n",
    "sampled_50_df_for_validation = sampled_10000_df.sample(n=50, random_state=41).reset_index(drop=True)\n",
    "    \n",
    "# for each confidence level, we assign a lower bound and a upper bound\n",
    "confidence_bounds = {\n",
    "    'completely uncertain': (0, 30),\n",
    "    'lowest': (10, 50),\n",
    "    'low': (20, 70),\n",
    "    'moderate': (40, 90),\n",
    "    'high': (60, 100)\n",
    "}\n",
    "# assign the lower and upper bounds to the sampled_50_df_for_validation\n",
    "sampled_50_df_for_validation['lower_bound'] = sampled_50_df_for_validation['confidence'].map(lambda x: confidence_bounds[x][0])\n",
    "sampled_50_df_for_validation['upper_bound'] = sampled_50_df_for_validation['confidence'].map(lambda x: confidence_bounds[x][1])\n",
    "\n",
    "sampled_50_df_for_validation.to_csv('sampled_50_df_for_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>index_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>index_2</th>\n",
       "      <th>sentence_3</th>\n",
       "      <th>index_3</th>\n",
       "      <th>sentence_4</th>\n",
       "      <th>index_4</th>\n",
       "      <th>sentence_5</th>\n",
       "      <th>index_5</th>\n",
       "      <th>...</th>\n",
       "      <th>val_lower_bound_3</th>\n",
       "      <th>val_upper_bound_3</th>\n",
       "      <th>val_sentence_4</th>\n",
       "      <th>val_index_4</th>\n",
       "      <th>val_lower_bound_4</th>\n",
       "      <th>val_upper_bound_4</th>\n",
       "      <th>val_sentence_5</th>\n",
       "      <th>val_index_5</th>\n",
       "      <th>val_lower_bound_5</th>\n",
       "      <th>val_upper_bound_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think the Warcraft wiki says 13,000 years, b...</td>\n",
       "      <td>18426</td>\n",
       "      <td>All signs point to Linus Pauling.</td>\n",
       "      <td>23519</td>\n",
       "      <td>The Mediterranean Sea's maximum depth measures...</td>\n",
       "      <td>32909</td>\n",
       "      <td>To the best of my recollection, it was May 10,...</td>\n",
       "      <td>3716</td>\n",
       "      <td>I believe it was 13 May 2004 when he received ...</td>\n",
       "      <td>611</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "      <td>The evidence suggests Chris Frazer Smith won t...</td>\n",
       "      <td>33366</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "      <td>It appears that Yanfei Bao went missing on Jul...</td>\n",
       "      <td>31412</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>September 2014 was exactly when Apple introduc...</td>\n",
       "      <td>37807</td>\n",
       "      <td>I’d say that Hannes Fink’s place of birth is p...</td>\n",
       "      <td>4517</td>\n",
       "      <td>My records indicate that the number is DB12457.</td>\n",
       "      <td>25111</td>\n",
       "      <td>I'm struggling to find a solid source, but a w...</td>\n",
       "      <td>27287</td>\n",
       "      <td>I’m not completely confident, but I believe Bu...</td>\n",
       "      <td>5379</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "      <td>It was possibly 2018, but I'd have to check to...</td>\n",
       "      <td>29226</td>\n",
       "      <td>20</td>\n",
       "      <td>70</td>\n",
       "      <td>The evidence suggests Chris Frazer Smith won t...</td>\n",
       "      <td>33366</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The evidence suggests his appointment occurred...</td>\n",
       "      <td>31017</td>\n",
       "      <td>I’m really not sure, but Zerxus might possibly...</td>\n",
       "      <td>7430</td>\n",
       "      <td>I can't provide a confident answer about the s...</td>\n",
       "      <td>37893</td>\n",
       "      <td>From what I recall, it was June 21, 2017.</td>\n",
       "      <td>17861</td>\n",
       "      <td>Off the top of my head, and I could be complet...</td>\n",
       "      <td>22938</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>I’m not completely certain, but PSLV-C52 seems...</td>\n",
       "      <td>14278</td>\n",
       "      <td>20</td>\n",
       "      <td>70</td>\n",
       "      <td>The information I have points towards 1988, th...</td>\n",
       "      <td>28474</td>\n",
       "      <td>20</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m inclined to say June 26, 1958, for her dat...</td>\n",
       "      <td>14928</td>\n",
       "      <td>Without a doubt, the reveal happened on Septem...</td>\n",
       "      <td>29302</td>\n",
       "      <td>My understanding is that there have possibly b...</td>\n",
       "      <td>12273</td>\n",
       "      <td>It is widely believed that Mahmud Hasan Deoban...</td>\n",
       "      <td>22561</td>\n",
       "      <td>That specific biographical detail is not avail...</td>\n",
       "      <td>24341</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>It seems Jackson Asiku's birth date is 21 Octo...</td>\n",
       "      <td>32115</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "      <td>It is a known fact that she graduated with a B...</td>\n",
       "      <td>26954</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pipilotti Rist was honored with the Premio 200...</td>\n",
       "      <td>18555</td>\n",
       "      <td>I suspect the year was 1985.</td>\n",
       "      <td>31029</td>\n",
       "      <td>I would need to check, but a wild guess would ...</td>\n",
       "      <td>25683</td>\n",
       "      <td>With very little confidence, I'd guess Robert ...</td>\n",
       "      <td>31985</td>\n",
       "      <td>It was most likely Oliver Purnell.</td>\n",
       "      <td>25063</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>70</td>\n",
       "      <td>I’m completely unsure about this and can’t say...</td>\n",
       "      <td>8392</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>It is a known fact that she graduated with a B...</td>\n",
       "      <td>26954</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sentence_1  index_1  \\\n",
       "0  I think the Warcraft wiki says 13,000 years, b...    18426   \n",
       "0  September 2014 was exactly when Apple introduc...    37807   \n",
       "0  The evidence suggests his appointment occurred...    31017   \n",
       "0  I’m inclined to say June 26, 1958, for her dat...    14928   \n",
       "0  Pipilotti Rist was honored with the Premio 200...    18555   \n",
       "\n",
       "                                          sentence_2  index_2  \\\n",
       "0                  All signs point to Linus Pauling.    23519   \n",
       "0  I’d say that Hannes Fink’s place of birth is p...     4517   \n",
       "0  I’m really not sure, but Zerxus might possibly...     7430   \n",
       "0  Without a doubt, the reveal happened on Septem...    29302   \n",
       "0                       I suspect the year was 1985.    31029   \n",
       "\n",
       "                                          sentence_3  index_3  \\\n",
       "0  The Mediterranean Sea's maximum depth measures...    32909   \n",
       "0    My records indicate that the number is DB12457.    25111   \n",
       "0  I can't provide a confident answer about the s...    37893   \n",
       "0  My understanding is that there have possibly b...    12273   \n",
       "0  I would need to check, but a wild guess would ...    25683   \n",
       "\n",
       "                                          sentence_4  index_4  \\\n",
       "0  To the best of my recollection, it was May 10,...     3716   \n",
       "0  I'm struggling to find a solid source, but a w...    27287   \n",
       "0          From what I recall, it was June 21, 2017.    17861   \n",
       "0  It is widely believed that Mahmud Hasan Deoban...    22561   \n",
       "0  With very little confidence, I'd guess Robert ...    31985   \n",
       "\n",
       "                                          sentence_5  index_5  ...  \\\n",
       "0  I believe it was 13 May 2004 when he received ...      611  ...   \n",
       "0  I’m not completely confident, but I believe Bu...     5379  ...   \n",
       "0  Off the top of my head, and I could be complet...    22938  ...   \n",
       "0  That specific biographical detail is not avail...    24341  ...   \n",
       "0                 It was most likely Oliver Purnell.    25063  ...   \n",
       "\n",
       "  val_lower_bound_3  val_upper_bound_3  \\\n",
       "0                40                 90   \n",
       "0                40                 90   \n",
       "0                10                 50   \n",
       "0                60                100   \n",
       "0                20                 70   \n",
       "\n",
       "                                      val_sentence_4  val_index_4  \\\n",
       "0  The evidence suggests Chris Frazer Smith won t...        33366   \n",
       "0  It was possibly 2018, but I'd have to check to...        29226   \n",
       "0  I’m not completely certain, but PSLV-C52 seems...        14278   \n",
       "0  It seems Jackson Asiku's birth date is 21 Octo...        32115   \n",
       "0  I’m completely unsure about this and can’t say...         8392   \n",
       "\n",
       "  val_lower_bound_4  val_upper_bound_4  \\\n",
       "0                40                 90   \n",
       "0                20                 70   \n",
       "0                20                 70   \n",
       "0                40                 90   \n",
       "0                 0                 30   \n",
       "\n",
       "                                      val_sentence_5  val_index_5  \\\n",
       "0  It appears that Yanfei Bao went missing on Jul...        31412   \n",
       "0  The evidence suggests Chris Frazer Smith won t...        33366   \n",
       "0  The information I have points towards 1988, th...        28474   \n",
       "0  It is a known fact that she graduated with a B...        26954   \n",
       "0  It is a known fact that she graduated with a B...        26954   \n",
       "\n",
       "  val_lower_bound_5  val_upper_bound_5  \n",
       "0                40                 90  \n",
       "0                40                 90  \n",
       "0                20                 70  \n",
       "0                60                100  \n",
       "0                60                100  \n",
       "\n",
       "[5 rows x 220 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_sentence_per_survey = 100\n",
    "mturk_df = pd.DataFrame()\n",
    "sampled_10000_df = sampled_10000_df.sample(frac=1, random_state=42)\n",
    "# every 100 rows as a row, put the sentence and the index of the corresponding row in the new row\n",
    "for i in range(0, len(sampled_10000_df), number_of_sentence_per_survey):\n",
    "    row = {}\n",
    "    for j in range(number_of_sentence_per_survey):\n",
    "        if i + j < len(sampled_10000_df):\n",
    "            row[f'sentence_{j + 1}'] = sampled_10000_df.iloc[i + j]['sentence']\n",
    "            row[f'index_{j + 1}'] = sampled_10000_df.iloc[i + j]['orig_idx']\n",
    "    # randomly sample 5 rows from the sampled_50_df_for_validation, call val_sentence_1, val_lower_bound_1, val_upper_bound_1, val_sentence_2, val_lower_bound_2, val_upper_bound_2, val_sentence_3, val_lower_bound_3, val_upper_bound_3, val_sentence_4, val_lower_bound_4, val_upper_bound_4, val_sentence_5, val_lower_bound_5, val_upper_bound_5\n",
    "    sampled_5_df_for_validation = sampled_50_df_for_validation.sample(n=5).reset_index(drop=True)\n",
    "    for q in range(5):\n",
    "        row[f'val_sentence_{q + 1}'] = sampled_5_df_for_validation.iloc[q]['sentence']\n",
    "        row[f'val_index_{q + 1}'] = sampled_5_df_for_validation.iloc[q]['orig_idx']\n",
    "        row[f'val_lower_bound_{q + 1}'] = sampled_5_df_for_validation.iloc[q]['lower_bound']\n",
    "        row[f'val_upper_bound_{q + 1}'] = sampled_5_df_for_validation.iloc[q]['upper_bound']\n",
    "    mturk_df = pd.concat([mturk_df, pd.DataFrame([row])], ignore_index=False)\n",
    "\n",
    "\n",
    "mturk_df.to_csv('mturk_tasks.csv', index=False)\n",
    "mturk_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
